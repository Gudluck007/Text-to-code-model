from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer
from datasets import load_dataset
import torch
from transformers import Seq2SeqTrainingArguments

#getting the dataset
dataset = load_dataset("sahil2801/CodeAlpaca-20k")


#Reduce the number of features in the train dataset to 5,000
dataset["train"] = dataset["train"].select(range(5000))
dataset["validation"] = dataset["train"].select(range(5000))
dataset["test"] = dataset["train"].select(range(5000))

print(dataset)


tokenizer = AutoTokenizer.from_pretrained("Salesforce/codet5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("Salesforce/codet5-base")

#training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=False,
    logging_dir="./logs",
)

training_args.generation_max_length = 512

#Preprocess the data
def preprocess_function(examples):
    inputs = examples["instruction"]
    targets = examples["output"]
    inputs = [f"translate English to code: {input_text}" for input_text in inputs]
    model_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=512)
    with tokenizer.as_target_tokenizer():
        targets = tokenizer(targets, padding=True, truncation=True, max_length=512)

    model_inputs["labels"] = targets["input_ids"]
    return model_inputs

preprocessed_dataset = dataset.map(preprocess_function, batched=True)

#the data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

#Fine-tune the model
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=preprocessed_dataset["train"],
    eval_dataset=preprocessed_dataset["validation"],
    data_collator=data_collator,
)

trainer.train()
output_dir = "./finetuned_model"

#Save the fine-tuned model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)
tokenizer = AutoTokenizer.from_pretrained(output_dir)
generation_params = {
    "max_length": 512,
    "num_beams": 4,
    "length_penalty": 0.6,
    "no_repeat_ngram_size": 3,
    "early_stopping": True
}

#Generate code from text
input_text = "Write a function that can add two numbers and will return the value gotten after it has added the two numbers "
input_text = f"translate English to code: {input_text}"
input = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors="pt")
output = model.generate(input["input_ids"], **generation_params)
decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)

print(decoded_output)
